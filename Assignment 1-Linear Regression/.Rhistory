trControl=trainControl(method="cv", number=5),verbose=FALSE,data=training)
pred_rt <- predict(fit_rf, testing$classe)
dim(testing)
pred_rt <- predict(fit_rf, testing)
confusionMatrix(pred_rt,testing$classe)$overall[[1]]
confusionMatrix(pred_rp,testing$classe)$overall[[1]] #Accuracy: 0.6615
confusionMatrix(pred_gbm,testing$classe)$overall[[1]]
#gam
fit_gam <- train(classe ~ ., preProcess= c("center","scale"), method="gam",
trControl=trainControl(method="cv", number=5),data=training)
#lda
fit_lda <- train(classe ~ ., preProcess= c("center","scale"), method="lda",
trControl=trainControl(method="cv", number=5),data=training)
pred_lda <- predict(fit_lda,testing$classe)
pred_lda <- predict(fit_lda,testing)
confusionMatrix(pred_lda,testing$classe)$overall[[1]]
confusionMatrix(pred_lda,testing$classe)
fit_svm <-svm(classe~., data=training)
pred_svm <- predict(fit_svm,testing$classe)
pred_svm <- predict(fit_svm,testing)
confusionMatrix(pred_svm,testing$classe)$overall[[1]]
print(fit_gbm)
print(fit_rpart)
ImpVar <- varImp(fit_rf)
ImpVar
print(fit_rf$finalModel)
names(training)
#Remove user-name
pml_train <- subset(pml_train, select=-c(X,cvtd_timestamp,user_name))
pml_test <- subset(pml_test, select=-c(X,cvtd_timestamp,user_name))
names(pml_train)
pml_train <- read.csv("/Users/andrewhu/Documents/GitHub/Coursera_DataScience_JHU/Practical Machine Learning/Data/pml-training.csv")
pml_test <- read.csv("/Users/andrewhu/Documents/GitHub/Coursera_DataScience_JHU/Practical Machine Learning/Data/pml-testing.csv")
#Remove user-name
pml_train <- subset(pml_train, select=-c(X,cvtd_timestamp,user_name))
pml_test <- subset(pml_test, select=-c(X,cvtd_timestamp,user_name))
pml_train <- read.csv("/Users/andrewhu/Documents/GitHub/Coursera_DataScience_JHU/Practical Machine Learning/Data/pml-training.csv")
pml_test <- read.csv("/Users/andrewhu/Documents/GitHub/Coursera_DataScience_JHU/Practical Machine Learning/Data/pml-testing.csv")
#Remove user-name
pml_train <- subset(pml_train, select=-c(X,cvtd_timestamp,user_name))
#Remove user-name
pml_train <- subset(pml_train, select=-c(X,cvtd_timestamp,user_name))
pml_train <- read.csv("/Users/andrewhu/Documents/GitHub/Coursera_DataScience_JHU/Practical Machine Learning/Data/pml-training.csv")
pml_test <- read.csv("/Users/andrewhu/Documents/GitHub/Coursera_DataScience_JHU/Practical Machine Learning/Data/pml-testing.csv")
#Remove user-name
pml_train <- subset(pml_train, select=-c(X,cvtd_timestamp,user_name))
names(pml_train)
head(pml_train)
pml_train <- read.csv("/Users/andrewhu/Documents/GitHub/Coursera_DataScience_JHU/Practical Machine Learning/Data/pml-training.csv")
pml_test <- read.csv("/Users/andrewhu/Documents/GitHub/Coursera_DataScience_JHU/Practical Machine Learning/Data/pml-testing.csv")
#Remove user-name
pml_train <- subset(pml_train, select=-c(X,cvtd_timestamp,user_name,raw_timestamp_part_1,raw_timestamp_part_2))
names(pml_train)
nzv<- nearZeroVar(pml_train,saveMetrics=T)
pml_train<- pml_train[,nzv$nzv==FALSE]
#Remove NA more than 85%
pml_train <-pml_train[, colMeans(is.na(pml_train)) <=.15]
---
#Data Splitting:
inTrain <- createDataPartition(y=pml_train$classe,
p=0.75, list=FALSE)
---
#Data Splitting:
inTrain <-createDataPartition(y=pml_train$classe,
p=0.75, list=FALSE)
---
#Data Splitting:
inTrain <-createDataPartition(y=pml_train$classe,p=0.75, list=FALSE)
inTrain <-createDataPartition(y=pml_train$classe,p=0.75, list=FALSE)
training <- pml_train[inTrain,]
testing <- pml_train[-inTrain,]
dim(training)
dim(testing)
#Fit-Control parameters
fitControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 1)
#gbm
fit_gbm <- train(classe~., method="gbm",preProcess= c("center","scale"),data=training,trControl=fitControl,verbose=FALSE)
print(fit_gbm$finalmodel)
print(fit_gbm$finalModel)
print(fit_gbm)
pred_gbm <- predict(fit_gbm,testing)
confusionMatrix(pred_gbm,testing$classe)$overall[[1]] #Accuracy: 0.9995
varImp(fit_gbm)
varImp(pred_gbm)
head(training)
#classification tree
fit_rpart <- train(classe ~ ., preProcess= c("center","scale"), method="rpart",data=training)
print(fit_rpart)
pred_rp <- predict(fit_rpart,testing)
confusionMatrix(pred_rp,testing$classe)$overall[[1]] #Accuracy: 0.6615
fancyRpartPlot(fit_rpart$finalModel)
library(rattle)
fancyRpartPlot(fit_rpart$finalModel)
fancyRpartPlot(fit_rpart$finalModel)
#randomForest
fit_rf <- train(classe ~ ., preProcess= c("center","scale"), method="rf",
trControl=fitControl,verbose=FALSE,data=training)
library(caret)
library(kernlab)
library(ISLR)
library(ggplot2)
library(gridExtra)
library(Hmisc)
library(elasticnet)
library(e1071)
library(randomForest)
library(rattle)
pml_train <- read.csv("/Users/andrewhu/Documents/GitHub/Coursera_DataScience_JHU/Practical Machine Learning/Data/pml-training.csv")
pml_test <- read.csv("/Users/andrewhu/Documents/GitHub/Coursera_DataScience_JHU/Practical Machine Learning/Data/pml-testing.csv")
#print our rf model
print(fit_rf$finalModel)
#print our rf model
print(fit_rf)
#Predict the testing data set using our rf model
pred_rt <- predict(fit_rf, testing)
#Use confusion matrix to check accuracy
confusionMatrix(pred_rt,testing$classe)$overall[[1]]  #Accuracy: 0.9997
#Check the importance of our variables
ImpVar <- varImp(fit_rf)
ImpVar
#Plot
plot(fit_rf)
plot(fit_rf)
#lda
fit_lda <- train(classe ~ ., preProcess= c("center","scale"), method="lda",
trControl=trainControl(method="cv", number=5),data=training)
pred_lda <- predict(fit_lda,testing)
confusionMatrix(pred_lda,testing$classe)$overall[[1]]
plot(fit_lda)
#svm
fit_svm <-svm(classe~., data=training)
pred_svm <- predict(fit_svm,testing)
confusionMatrix(pred_svm,testing$classe)$overall[[1]]
#gbm
fit_gbm <- train(classe~., method="gbm",preProcess= c("center","scale"),data=training,trControl=fitControl,verbose=FALSE)
print(fit_gbm)
pred_gbm <- predict(fit_gbm,testing)
confusionMatrix(pred_gbm,testing$classe)$overall[[1]]
comb_df <-data.frame(pred_gbm,pred_lda,pred_rf,classe=testing$classe)
#Predict the testing data set using our rf model
pred_rf <- predict(fit_rf, testing)
comb_df <-data.frame(pred_gbm,pred_lda,pred_rf,classe=testing$classe)
fit_comb <- train(classe~. , method="gam",preProcess= c("center","scale"),data=comb_df,trControl=fitControl,verbose=FALSE)
pred_comb <- predict(fit_comb,comb_df)
confusionMatrix(pred_comb,testing$classe)$overall[[1]]
fit_comb <- train(classe~. , method="rf",preProcess= c("center","scale"),data=comb_df,trControl=fitControl,verbose=FALSE)
pred_comb <- predict(fit_comb,comb_df)
confusionMatrix(pred_comb,testing$classe)$overall[[1]]
predict(fit_comb,pml_test)
preddd<- predict(fit_comb,pml_test)
predict(fit_rf ,pml_test)
comb_df <-data.frame(pred_gbm,pred_lda,pred_rf,classe=pml_test$classe)
pred_gbm <- predict(fit_gbm,pml_test)
comb_df <-data.frame(pred_gbm,pred_lda,pred_rf,classe=testing$classe)
pred_gbm <- predict(fit_gbm,testing)
comb_df <-data.frame(pred_gbm,pred_lda,pred_rf,classe=testing$classe)
#Stack all the models using random Forest
set.seed(777)
fit_comb <- train(classe~. , method="rf",preProcess= c("center","scale"),data=comb_df,trControl=fitControl,verbose=FALSE)
pred_comb <- predict(fit_comb,comb_df)
confusionMatrix(pred_comb,testing$classe)$overall[[1]]
pred_rf2 <- predict(fit_rf,pml_test)
pred_gbm2 <- predict(fit_gbm,pml_test)
pred_lda2 <- predict(fit_lda,pml_test)
comb_df2 <-data.frame(pred_gbm2,pred_lda2,pred_rf2,classe=pml_test$classe)
pred_rf2 <- predict(fit_rf,pml_test)
pred_rf2
pred_gbm2
pred_lda2
pml_test
summary(pml_test)
dim(pml_test)
comb_df2 <-data.frame(pred_gbm2,pred_lda2,pred_rf2,classe=pml_test$classe)
pred_rf2 <- predict(fit_rf,pml_test)
pred_gbm2 <- predict(fit_gbm,pml_test)
pred_lda2 <- predict(fit_lda,pml_test)
comb_df2 <-data.frame(pred_gbm2,pred_lda2,pred_rf2,classe=pml_test$classe)
comb_df
testing
dim(testing)
dim(pred_gbm)
length(pred_gbm)
length(pred_gbm2)
length(pred_lda2)
length(pred_rf2)
length(pml_test)
dim(testing)
length(testing)
length(pml_test)
nrow(pml_test)
comb_df2 <-data.frame(pred_gbm2,pred_lda2,pred_rf2,classe=pml_test$classe)
comb_df <-data.frame(pred_gbm,pred_lda,pred_rf,classe=testing$classe)
pred_rf2 <- predict(fit_rf,pml_test)
pred_gbm2 <- predict(fit_gbm,pml_test)
pred_lda2 <- predict(fit_lda,pml_test)
pred_lda2
pml_test$classe
pml_test$classe
summary(pml_test$classe)
head(pml_test)
pred_rf2 <- predict(fit_comb,pml_test)
pred_rf2 <- predict(fit_rf,pml_test)
comb_df2 <-data.frame(pred_gbm2,pred_lda2,pred_rf2)
comb_predict <- predict(fit_comb,comb_df2)
predict(fit_rf,pml_test)
predict(fit_comb, pml_test)
predict(fit_rf,pml_test)
fit_rf
fit_comb
fit_rf
fit_rf
fit_comb
predict(fit_gbm,pml_test)
comb_df <-data.frame(pred_gbm,pred_lda,pred_rf,classe=training$classe)
comb_df <-data.frame(pred_gbm,pred_lda,pred_rf,classe=testing$classe)
str(pml_train)
summary(pml_train)
shiny::runApp('Documents/GitHub/Coursera_DataScience_JHU/Developing Data Products/Final Project')
runApp('Documents/GitHub/Coursera_DataScience_JHU/Developing Data Products/Final Project')
library(dplyr)
?select
runApp('Documents/GitHub/Coursera_DataScience_JHU/Developing Data Products/Final Project')
runApp('Documents/GitHub/Coursera_DataScience_JHU/Developing Data Products/Final Project')
enblogrul <- file("en_US.blogs.txt", open="rb")
setwd("/users/andrewhu/desktop/final/en_US")
enblogrul <- file("en_US.blogs.txt", open="rb")
entwurl <- file("en_US.twitter.txt", open="rb")
enneurl <- file("en_US.news.txt", open="rb")
en_blogs<- readLines(enblogrul, encoding = "UTF-8", skipNul=TRUE)
en_tw<- readLines(entwurl, encoding = "UTF-8", skipNul=TRUE)
en_news<- readLines(enneurl, encoding = "UTF-8", skipNul=TRUE)
# File Inofo
file.info("en_US.blogs.txt")
file.info("en_US.news.txt")
file.info("en_US.twitter.txt")
#Lines
length(en_blogs)
length(en_tw)
length(en_news)
# Numbers of words
sum(stri_count_words(blogs))
library(stringi)
library(NLP)
library(tm)
library(ggplot2)
# Numbers of words
sum(stri_count_words(blogs))
# Numbers of words
sum(stri_count_words(en_blogs))
?stri_count_words
set.seed(525)
mini_tw <- sample(en_tw, size = 6000, replace = FALSE)
mini_blogs <- sample(en_blogs, size = 6000, replace = F)
mini_news <- sample(en_news, size = 6000, replace = F)
all <- c(mini_tw,mini_blogs,mini_news)
writeLines(sampleTotal, "/users/andrewhu/Desktop/final/en_US/sample.txt")
writeLines(all, "/users/andrewhu/Desktop/final/en_US/sample.txt")
install.packages("quanteda")
library(quanteda)
tok <- tokenize(toLower(all), removePunct= T, removeNumbers= T, simplify=T)
?tokenize
tok <- tokens(toLower(all), removePunct= T, removeNumbers= T, simplify=T)
?tokens
tok <- tokens(all, removePunct= T, removeNumbers= T, simplify=T)
tok <- tokens(all, removePunctuation= T, removeNumbers= T, simplify=T)
tok <- tokens(all, removePunctuation= T, removeNumbers= T, remove_twitter=T)
dfm1 <-dfm(tok, stem=T)
dfm1
print(dfm1)
dfm1 <-dfm(tok,verbose=T,toLower=T, stem=T)
top20_df <- data.frame(word=names(top20), freq=top20)
top20<- topfeatures(dfm1,20) # for words
top20_df <- data.frame(word=names(top20), freq=top20)
g <- ggplot(top20_df[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identify")
g + geom_bar(stat="identity")
dfm1 <-dfm(tok,verbose=T,toLower=T, stem=T, ignoredFeatures=c("will", stopwords("english")), valuetype=c("glob","regex","fixed"))
top20<- topfeatures(dfm1,20) # for words
top20_df <- data.frame(word=names(top20), freq=top20)
g <- ggplot(top20_df[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identity")
dfm1 <-dfm(tok,verbose=T,toLower=T, stem=T, ignoredFeatures=c("will", stopwords("english")), valuetype=c("glob","regex","fixed"), keptFeatures=NULL)
top20<- topfeatures(dfm1,20) # for words
top20_df <- data.frame(word=names(top20), freq=top20)
g <- ggplot(top20_df[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identity")
?dfm
tok <- tokens(all, verbose=T, removePunctuation= T, removeNumbers= T,remove_symbols=T)
tok <- tokens(all, verbose=T, removePunctuation= T, removeNumbers= T,remove_symbols=T,remove_twitter=T)
dfm1 <-dfm(tok,verbose=T,toLower=T, stem=T, ignoredFeatures=c("will", stopwords("english")), valuetype=c("glob","regex","fixed"), keptFeatures=NULL)
top20<- topfeatures(dfm1,20) # for words
top20_df <- data.frame(word=names(top20), freq=top20)
g <- ggplot(top20_df[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identity")
dfm1 <-dfm(tok,verbose=T,toLower=T, stem=T, ignoredFeatures=c("will", stopwords("english")), valuetype=c("glob","regex","fixed"), keptFeatures=NULL,removePunctuation=T)
top20<- topfeatures(dfm1,20) # for words
top20_df <- data.frame(word=names(top20), freq=top20)
g <- ggplot(top20_df[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identity")
dfm1 <-dfm(tok,verbose=T,toLower=T,removeNumbers=T,removePunct=T,removeSeparators=T,removeTwitter=T, stem=T, ignoredFeatures=c("will", stopwords("english")), valuetype=c("glob","regex","fixed"), keptFeatures=NULL)
top20<- topfeatures(dfm1,20) # for words
top20_df <- data.frame(word=names(top20), freq=top20)
g <- ggplot(top20_df[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identity")
con <- file(sample.txt, open="r")
con <- file("sample.txt"", open="r")
con <- file("sample.txt", open="r")
getwd()
setwd("/users/andrewhu/desktop/final/en_US")
con <- file("sample.txt", open="r")
con <- file("sample.txt", open="r")
con <- file("/users/andrewhu/Desktop/final/en_US/sample.txt", open="r")
con <- file("/users/andrewhu/Desktop/final/en_US/sample.txt", open="r")
sam<- readLines(con, encoding = "UTF-8", n = nlines, warn = TRUE, skipNul = TRUE)
con <- file("/users/andrewhu/Desktop/final/en_US/sample.txt", open="r")
sam<- readLines(con, encoding = "UTF-8", n = -1L, warn = TRUE, skipNul = TRUE)
close(con)
sam<- gsub("<(U\\+([[:alnum:]]{4}))>", " ", sam, ignore.case = T)
sam <- gsub("â€™", "’", sam, ignore.case = T)
sam <- gsub("\\_", "", sam, ignore.case = T)
invisible(sam)
summary(sam)
tok <- tokens(sam, verbose=T, removePunctuation= T, removeNumbers= T,remove_symbols=T,remove_twitter=T)
dfm1 <-dfm(tok,verbose=T,toLower=T,removeNumbers=T,removePunct=T,removeSeparators=T,removeTwitter=T, stem=T, ignoredFeatures=c("will", stopwords("english")), valuetype=c("glob","regex","fixed"), keptFeatures=NULL)
top20<- topfeatures(dfm1,20) # for words
top20_df <- data.frame(word=names(top20), freq=top20)
g <- ggplot(top20_df[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identity")
tok <- tokens(sam, verbose=T, removePunctuation= T, removeNumbers= T,remove_symbols=T,remove_twitter=T)
dfm1 <-dfm(tok,verbose=T,toLower=T,removeNumbers=T,removePunct=T,removeSeparators=T,removeTwitter=T, stem=T, ignoredFeatures=c("will", stopwords("english")), valuetype=c("glob","regex","fixed"), keptFeatures=NULL)
top20<- topfeatures(dfm1,20) # for words
top20_df <- data.frame(word=names(top20), freq=top20,row.names = NULL)
g <- ggplot(top20_df[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identity")
tok <- tokens(sam, verbose=T, removePunctuation= T, removeNumbers= T,remove_symbols=T,remove_twitter=T)
dfm1 <-dfm(tok,verbose=T,toLower=T,removeNumbers=T,removePunct=T,removeSeparators=T,removeTwitter=T, stem=T, ignoredFeatures=c("will", stopwords("SMART")), valuetype=c("glob","regex","fixed"), keptFeatures=NULL)
top20<- topfeatures(dfm1,20) # for words
top20_df <- data.frame(word=names(top20), freq=top20,row.names = NULL)
g <- ggplot(top20_df[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identity")
length(all)
mini_tw <- sample(en_tw, size = length(en_tw)*0.1, replace = FALSE)
mini_blogs <- sample(en_blogs, size = length(en_blogs)*0.1, replace = F)
mini_news <- sample(en_news, size = length(en_news)*0.1, replace = F)
sample <- c(mini_tw,mini_blogs,mini_news)
writeLines(sample, "/users/andrewhu/Desktop/final/en_US/sample.txt")
length(sample)
con <- file("/users/andrewhu/Desktop/final/en_US/sample.txt", open="r")
sam<- readLines(con, encoding = "UTF-8", n = -1L, warn = TRUE, skipNul = TRUE)
close(con)
sam<- gsub("<(U\\+([[:alnum:]]{4}))>", " ", sam, ignore.case = T)
sam <- gsub("â€™", "’", sam, ignore.case = T)
sam <- gsub("\\_", "", sam, ignore.case = T)
invisible(sam)
tok <- tokens(sam, verbose=T, removePunctuation= T, removeNumbers= T,remove_symbols=T,remove_twitter=T)
dfm1 <-dfm(tok,verbose=T,toLower=T,removeNumbers=T,removePunct=T,removeSeparators=T,removeTwitter=T, stem=T, ignoredFeatures=c("will", stopwords("SMART")), valuetype=c("glob","regex","fixed"), keptFeatures=NULL)
top20<- topfeatures(dfm1,20) # for words
top20_df <- data.frame(word=names(top20), freq=top20,row.names = NULL)
g <- ggplot(top20_df[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identity")
dfm1 <-dfm(tok, verbose = TRUE, toLower = TRUE,
removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE,
removeTwitter = TRUE, stem = TRUE, ignoredFeatures = c("will", stopwords("english")),
keptFeatures = NULL, language = "english", thesaurus = NULL,
dictionary = NULL, valuetype = c("glob", "regex", "fixed"))
set.seed(525)
mini_tw <- sample(en_tw, size = 25000, replace = FALSE)
mini_blogs <- sample(en_blogs, size = 25000, replace = F)
mini_news <- sample(en_news, size = 25000, replace = F)
sample <- c(mini_tw,mini_blogs,mini_news)
writeLines(sample, "/users/andrewhu/Desktop/final/en_US/sample.txt")
length(sample)
rm(sam)
tok <- tokens(sample, verbose=T, removePunctuation= T, removeNumbers= T,remove_symbols=T,remove_twitter=T)
dfm1 <-dfm(tok, verbose = TRUE, toLower = TRUE,
removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE,
removeTwitter = TRUE, stem = TRUE, ignoredFeatures = c("will", stopwords("english")),
keptFeatures = NULL, language = "english", thesaurus = NULL,
dictionary = NULL, valuetype = c("glob", "regex", "fixed"))
top20<- topfeatures(dfm1,20) # for words
top20_df <- data.frame(word=names(top20), freq=top20,row.names = NULL)
g <- ggplot(top20_df[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identity")
plot(top20_df_n1[1:20,], max.words=100, colors = brewer.pal(6, "Dark2"), scale=c(8, .5))
#suppressWarnings(library(stringi))
#suppressWarnings(library(NLP))
suppressWarnings(library(ggplot2))
suppressWarnings(library(quanteda))
suppressWarnings(library(dplyr))
suppressWarnings(library(data.table))
#Set work directory
setwd("/users/andrewhu/desktop/final/en_US")
#Read text file
enblogrul <- file("en_US.blogs.txt", open="rb")
entwurl <- file("en_US.twitter.txt", open="rb")
enneurl <- file("en_US.news.txt", open="rb")
#Read in the lines
en_blogs<- readLines(enblogrul, encoding = "UTF-8", skipNul=TRUE)
en_tw<- readLines(entwurl, encoding = "UTF-8", skipNul=TRUE)
en_news<- readLines(enneurl, encoding = "UTF-8", skipNul=TRUE)
# File Info
file.info("en_US.blogs.txt")
file.info("en_US.news.txt")
file.info("en_US.twitter.txt")
# legnth of Lines for each file
length(en_blogs)
length(en_tw)
length(en_news)
# Numbers of words for each file
sum(stri_count_words(en_blogs))
suppressWarnings(library(stringi))
# Numbers of words for each file
sum(stri_count_words(en_blogs))
sum(stri_count_words(en_news))
set.seed(525)
mini_tw <- sample(en_tw, size = 25000, replace = F)
mini_blogs <- sample(en_blogs, size = 25000, replace = F)
mini_news <- sample(en_news, size = 25000, replace = F)
sample <- c(mini_tw,mini_blogs,mini_news)
# filter weird symbols
sample <- gsub("[^[:alpha:][:space:]']", " ", sample)
sample <- gsub("ã", "'", sample)
sample <- gsub("ð", "'", sample)
sample <- gsub(" #\\S*","", sample) #delete hastag
sample <- gsub("(f|ht)(tp)(s?)(://)(\\S*)", "", sample) # delete http links
sample <- gsub("[^0-9A-Za-z///' ]", "", sample) #delete special chars
#check the length for our sample
length(sample)
#corpus for the whole sample
corp <- corpus(sample)
#tokenization for the corpus
toks <- tokens(corp)
#generate ngrams for 1~3
unigram <- tokens_ngrams(toks, n=1)
bigram <- tokens_ngrams(toks, n=2)
trigram <- tokens_ngrams(toks, n=3)
#document frequency matrix for ngram 1~4
unigram_dfm <- dfm(unigram,  toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE,
removeSeparators = TRUE, removeSymbols = TRUE, removeTwitter = TRUE,
removeHyphens = TRUE, what="fasterword", ngrams=1)
bigram_dfm <- dfm(bigram,  toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE,
removeSeparators = TRUE, removeSymbols = TRUE, removeTwitter = TRUE,
removeHyphens = TRUE, what="fasterword", ngrams=2)
trigram_dfm <- dfm(trigram,  toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE,
removeSeparators = TRUE, removeSymbols = TRUE, removeTwitter = TRUE,
removeHyphens = TRUE, what="fasterword", ngrams=3)
#Create top features for each n gram
top20_n1<- topfeatures(unigram_dfm,20)
top20_n2<- topfeatures(bigram_dfm ,20)
top20_n3<- topfeatures(trigram_dfm,20)
#create data frame
top20_df_n1 <- data.frame(word=names(top20_n1), freq=top20_n1,row.names = NULL)
top20_df_n2 <- data.frame(word=names(top20_n2), freq=top20_n2,row.names = NULL)
top20_df_n3 <- data.frame(word=names(top20_n3), freq=top20_n3,row.names = NULL)
#Bar chart for ngram:1, top 20 features
g <- ggplot(top20_df_n1[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identity") + labs(x= "words", y= "freq")+coord_flip()
#Bar chart for ngram:2, top 20 features
g <- ggplot(top20_df_n2[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identity")+ labs(x= "words", y= "freq")+coord_flip()
#Bar chart for ngram:3, top 20 features
g <- ggplot(top20_df_n3[1:20,], aes(reorder(word,-freq),freq))
g + geom_bar(stat="identity") + labs(x= "words", y= "freq")+coord_flip()
plot(top20_df_n1[1:20,], max.words=100, colors = brewer.pal(6, "Dark2"), scale=c(8, .5))
install.packages("R.utils")
getwd()
setwd("/Users/andrewhu/Documents/GitHub/Machine-Learning/Class Practice/Assignment 1/Property.csv"")
dfd
df
setwd("/Users/andrewhu/Documents/GitHub/Machine-Learning/Class Practice/Assignment 1/Property.csv")
setwd("/Users/andrewhu/Documents/GitHub/Machine-Learning/Class Practice/Assignment 1")
prop <- read.csv("Property.csv")
library(VIF)
install.packages("VIF")
library(VIF)
library(VIF)
setwd("/Users/andrewhu/Documents/GitHub/Machine-Learning/Class Practice/Assignment 1-Linear Regression Practice")
prop <- read.csv("Property.csv")
summary(prop)
plot(prop$ListPrice)
round(cor(prop$ListPrice,prop$Age),digits = 2)
round(cor(prop$ListPrice,prop$SquareFeet),digits = 2)
round(cor(prop$ListPrice,prop$Beds),digits = 2)
round(cor(prop$ListPrice,prop$Baths),digits = 2)
round(cor(prop$Age,prop$SquareFeet),digits = 2)
round(cor(prop$Age,prop$Beds),digits = 2)
round(cor(prop$SquareFeet,prop$Beds),digits = 2)
round(cor(prop$SquareFeet,prop$Baths),digits = 2)
round(cor(prop$Beds,prop$Baths),digits = 2)
fit1 = lm(prop$ListPrice~prop$Age)
fit2 = lm(prop$ListPrice~prop$SquareFeet)
fit3 = lm(prop$ListPrice~prop$Beds)
fit4 = lm(prop$ListPrice~prop$Baths)
fit5= lm(ListPrice~Age+SquareFeet+Beds+Baths, prop)
vif(fit5)
library(car)
install.packages("car")
setwd("/Users/andrewhu/Documents/GitHub/Machine-Learning/Class Practice/Assignment 2")
prop <- read.csv("Property.csv")
adv <- read.csv("Advertising.csv")
birth <- read.csv("Birth")
birth <- read.csv("Birth.csv")
library(car)
setwd("/Users/andrewhu/Documents/GitHub/Machine-Learning/Class Practice/Assignment 2")
prop <- read.csv("Property.csv")
adv <- read.csv("Advertising.csv")
birth <- read.csv("Birth.csv")
relev <- relevel(prop$County, ref="Passaic")
faccounty <- factor(relev)
fit = lm(ListPrice ~Age + SquareFeet + Beds + Baths + faccounty, prop)
